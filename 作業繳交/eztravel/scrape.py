import time
import json
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- Tab è¨­å®š ---
TAB_MAPPING = {
    "recommend": "ç²¾é¸æ¨è–¦",
    "hokuriku": "åŒ—é™¸",
    "hokkaido": "åŒ—æµ·é“",
    "tohoku": "æ±åŒ—",
    "tokyo": "æ±äº¬",
    "kansai": "é—œè¥¿",
    "kyushu": "ä¹å·",
    "shikoku": "å››åœ‹",
    "okinawa": "æ²–ç¹©",
    "kaohsiung": "é«˜é›„å‡ºç™¼"
}

def setup_driver():
    print("å•Ÿå‹•ç€è¦½å™¨ä¸­...")
    chrome_options = Options()
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1600,1200")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
    
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        return driver
    except Exception as e:
        print(f"ç€è¦½å™¨å•Ÿå‹•å¤±æ•—: {e}")
        return None

def scrape_data():
    driver = setup_driver()
    if not driver: return {}

    url = "https://www.eztravel.com.tw/"
    final_database = {}

    try:
        print(f"å‰å¾€ç¶²å€: {url}")
        driver.get(url)
        time.sleep(5) 

        # 1. å®šä½ Tabs_wrapper ä¸¦å–å¾—ã€Œç´…ç·šåº§æ¨™ã€
        print("æ­£åœ¨è¨­å®šåº§æ¨™åŸºæº–ç·š...")
        tabs_y_limit = 0
        try:
            # æ‰¾åˆ°æŒ‰éˆ•å€å¡Š
            tabs_wrapper = driver.find_element(By.XPATH, "//div[contains(@class, 'Tabs_wrapper')]")
            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", tabs_wrapper)
            time.sleep(2)
            
            # å–å¾—å®ƒçš„ Y åº§æ¨™ (é«˜åº¦)
            # æˆ‘å€‘åªæŠ“é€™å€‹é«˜åº¦ "ä»¥ä¸‹" çš„æ±è¥¿
            tabs_y_limit = tabs_wrapper.location['y'] + tabs_wrapper.size['height']
            print(f"âœ… åŸºæº–ç·šè¨­å®šç‚º Y > {tabs_y_limit} (æ’é™¤ä¸Šæ–¹æ‰€æœ‰å»£å‘Š)")
            
        except:
            print("âŒ åš´é‡éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° Tabs å€å¡Šï¼Œç„¡æ³•è¨­å®šéæ¿¾ç·š")
            return {}

        # 2. é–‹å§‹æŠ“å–
        for key, tab_text in TAB_MAPPING.items():
            print(f"\nğŸ” è™•ç† Tab: [{tab_text}]")
            items_list = []
            
            try:
                # [æ­¥é©Ÿ A] é»æ“Š Tab
                tab_xpath = f"//div[contains(@class, 'Tabs_wrapper')]//li//a[contains(text(), '{tab_text}')]"
                
                found_tabs = driver.find_elements(By.XPATH, tab_xpath)
                active_btn = None
                for btn in found_tabs:
                    if btn.is_displayed():
                        active_btn = btn
                        break
                
                if active_btn:
                    driver.execute_script("arguments[0].click();", active_btn)
                    print(f"  ğŸ‘† å·²é»æ“Šï¼Œç­‰å¾…å…§å®¹åˆ·æ–° (4ç§’)...")
                    time.sleep(4) # çµ¦å®ƒè¶³å¤ æ™‚é–“è¼‰å…¥
                    
                    # [æ­¥é©Ÿ B] æŠ“å–å…¨é æ‰€æœ‰å¯èƒ½çš„æ¨™é¡Œ (ä¸é™åˆ¶ç¯„åœ)
                    # ä½¿ç”¨ Selenium æ‰¾å…ƒç´ ï¼Œå› ç‚ºæˆ‘å€‘éœ€è¦æŸ¥å®ƒçš„åº§æ¨™
                    all_titles = driver.find_elements(By.CSS_SELECTOR, "h3.title")
                    
                    valid_count = 0
                    for title_ele in all_titles:
                        if valid_count >= 5: break
                        
                        try:
                            # [éæ¿¾ 1] ç‰©ç†åº§æ¨™æª¢æŸ¥
                            # å¦‚æœå¡ç‰‡åœ¨ç´…ç·šä¸Šé¢ -> è·³é (å®ƒæ˜¯ Sidebar)
                            if title_ele.location['y'] < tabs_y_limit:
                                continue
                            
                            # å¦‚æœå¤ªä¸‹é¢ (ä¾‹å¦‚ footer)ï¼Œä¹Ÿè·³é
                            if title_ele.location['y'] > tabs_y_limit + 6000:
                                continue

                            # [éæ¿¾ 2] çµæ§‹æª¢æŸ¥
                            # å¾€ä¸Šæ‰¾çˆ¶å±¤é€£çµ
                            card_link = title_ele.find_element(By.XPATH, "./ancestor::a")
                            
                            # å–å¾—å¡ç‰‡ HTML ä¸¦æ¸…ç†è…³æœ¬
                            card_html = card_link.get_attribute('outerHTML')
                            soup_card = BeautifulSoup(card_html, "html.parser")
                            for script in soup_card(["script", "style"]):
                                script.decompose()
                            
                            # æª¢æŸ¥å¿…è¦ç‰¹å¾µï¼šä¸€å®šè¦æœ‰ Description å’Œ Price Span
                            # Sidebar å»£å‘Šé€šå¸¸æ²’æœ‰ description class
                            desc_tag = soup_card.find("p", class_="description")
                            price_p = soup_card.find("p", class_="price")
                            
                            if desc_tag and price_p:
                                # æå–è³‡æ–™
                                title_text = soup_card.find("h3", class_="title").get_text(strip=True)
                                desc_text = desc_tag.get_text(strip=True)
                                
                                price_text = "0"
                                price_span = price_p.find("span")
                                if price_span:
                                    price_text = price_span.get_text(strip=True)
                                else:
                                    price_text = price_p.get_text(strip=True).replace("èµ·", "").replace("$", "").strip()
                                
                                # é€£çµ
                                href = soup_card.find("a", href=True)['href'] if soup_card.name != 'a' else soup_card['href']
                                full_link = "https://www.eztravel.com.tw" + href if not href.startswith("http") else href
                                
                                # åœ–ç‰‡
                                img_url = ""
                                img_tag = soup_card.find("img")
                                if img_tag:
                                    img_url = img_tag.get("src") or img_tag.get("data-src") or ""

                                items_list.append({
                                    "title": title_text,
                                    "desc": desc_text,
                                    "price": price_text,
                                    "img": img_url,
                                    "link": full_link
                                })
                                valid_count += 1
                                print(f"    âœ… æŠ“åˆ°: {title_text[:10]}... ${price_text}")
                        
                        except Exception as inner_e:
                            # å¿½ç•¥å–®ä¸€å¡ç‰‡çš„è§£æéŒ¯èª¤ (ä¾‹å¦‚å…ƒç´ æ¶ˆå¤±)
                            continue

                else:
                    print(f"  âš ï¸ æ‰¾ä¸åˆ° Tab æŒ‰éˆ•: {tab_text}")

            except Exception as e:
                print(f"  âŒ éŒ¯èª¤: {e}")

            final_database[key] = items_list

    except Exception as e:
        print(f"ç¨‹å¼ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        driver.quit()
        return final_database

def generate_js_file(data):
    print("\næ­£åœ¨ç”¢ç”Ÿ JS æª”æ¡ˆ...")
    js_content = "const tourDatabase = {\n"
    
    for key, items in data.items():
        js_content += f"  // {TAB_MAPPING.get(key, key)}\n"
        js_content += f"  {key}: [\n"
        for item in items:
            js_content += "    {\n"
            js_content += f"      title: {json.dumps(item['title'], ensure_ascii=False)},\n"
            js_content += f"      desc: {json.dumps(item['desc'], ensure_ascii=False)},\n"
            js_content += f"      price: \"{item['price']}\",\n"
            js_content += f"      img: \"{item['img']}\",\n"
            js_content += f"      link: \"{item['link']}\",\n"
            js_content += "    },\n"
        js_content += "  ],\n"
    
    js_content += "};\n"
    
    with open("tour_database.js", "w", encoding="utf-8") as f:
        f.write(js_content)
    print(f"âœ… tour_database.js å»ºç«‹å®Œæˆï¼")

if __name__ == "__main__":
    data = scrape_data()
    generate_js_file(data)